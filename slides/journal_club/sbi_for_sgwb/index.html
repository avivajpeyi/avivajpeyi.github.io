<!doctype html><html lang=en-us><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=generator content="Wowchemy 5.7.0 for Hugo"><link rel=manifest href=/manifest.webmanifest><link rel=icon type=image/png href=/media/icon_hu8bb3cd4a71e1097e37400a0f18d2a5e2_266862_32x32_fill_lanczos_center_3.png><link rel=apple-touch-icon type=image/png href=/media/icon_hu8bb3cd4a71e1097e37400a0f18d2a5e2_266862_192x192_fill_lanczos_center_3.png><link rel=canonical href=https://avivajpeyi.github.io/slides/journal_club/sbi_for_sgwb/><title>SBI for SGBW | Avi Vajpeyi</title><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/reveal.js@4.5.0/dist/reveal.min.css><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/reveal.js@4.5.0/dist/theme/white.min.css><link rel=stylesheet href=/css/libs/chroma/github-light.min.css></head><body><div class=reveal><div class=slides><section><h2 id=sbi-for-sgwb>SBI for SGWB</h2><p><em>Simulation based infernce for Stochastic GW background Analysis</em>
(Alvey+, 2023)</p><p><a href=https://arxiv.org/pdf/2309.07954.pdf target=_blank rel=noopener>arxiv</a> | <a href=https://github.com/undark-lab/swyft target=_blank rel=noopener>swyft</a> | <a href=https://github.com/avivajpeyi/dev_site/edit/main/content/slides/journal_club/sbi_for_sgwb/index.md target=_blank rel=noopener>edit</a></p><p>NZ Gravity Journal Club</p><p>Oct 26th, 2023</p></section><section><h2 id=summary>Summary</h2><ol><li>LISA &ldquo;Global fit&rdquo; + GW background</li><li>Alvey+&rsquo;s LISA SGWB model</li><li>Sim based inference + TMNRE</li><li>Results, Discussion + future work</li></ol></section><section><section data-shortcode-section><h2 id=lisa-data-analysis>LISA Data analysis</h2></section><section><h3 id=the-data>The data</h3><p><figure><div class="d-flex justify-content-center"><div class=w-100><img src=https://github.com/avivajpeyi/dev_site/assets/15642823/af1e82e7-f1bc-4306-856e-b11e245cadf3 alt=lisa_data loading=lazy data-zoomable></div></div></figure></p></section><section><h3 id=the-global-fit>The &ldquo;Global fit&rdquo;</h3><p>Analyze all the data, simultaneously, block-by-block</p><figure><div class="d-flex justify-content-center"><div class=w-100><img src=https://github.com/avivajpeyi/dev_site/assets/15642823/1577656f-3c97-43e9-bc4d-7da09c6686ce alt loading=lazy data-zoomable width=1300 height=350></div></div></figure><p>$&lt;10^5$ parameters in the full problem</p></section><section><h3 id=sgwb-estimation-methods>SGWB estimation methods</h3><table><thead><tr><th>Noise model</th><th>Signal model</th><th>Noise + Signal</th></tr></thead><tbody><tr><td><a href=https://arxiv.org/abs/1906.09027 target=_blank rel=noopener>Karnesis+ &lsquo;19</a></td><td><a href=https://arxiv.org/abs/2302.12573 target=_blank rel=noopener>Baghi+ &lsquo;23</a></td><td><a href=https://arxiv.org/abs/2011.05055 target=_blank rel=noopener>Boileau+ &lsquo;20</a></td></tr><tr><td><a href=https://arxiv.org/abs/1906.09244 target=_blank rel=noopener>Caprini+ &lsquo;19</a></td><td><a href=https://arxiv.org/abs/2308.01056 target=_blank rel=noopener>Muratore+ &lsquo;23</a></td><td><a href=https://arxiv.org/abs/2303.15929 target=_blank rel=noopener>Olaf+ &lsquo;23</a></td></tr><tr><td><a href=https://arxiv.org/abs/2004.01135 target=_blank rel=noopener>Pieroni+ &lsquo;20</a></td><td></td><td>Aimen+ (WIP)</td></tr></tbody></table><p>High precision reconstruction required to extract an SGWB signal</p></section></section><section><h2 id=alveys-sbi-approach-motivations>Alvey+&rsquo;s SBI approach motivations</h2><aside class=notes><ul><li>Current SGWB approaches use stochastic sampling methods (MCMC, Nested sampling)</li><li>These are not <em>robust</em> to foreground transient signals (e.g. massive BH mergers)</li><li>add more comlexities</li></ul></aside><ol><li>&lsquo;Marginal inference&rsquo; property</li><li>Likelihood &lsquo;free&rsquo; inference</li><li>More robust to foreground transient signals (e.g. massive BH mergers)</li></ol></section><section><section data-shortcode-section><h2 id=sbi>SBI</h2></section><section><h3 id=traditional-problem>Traditional problem</h3><p>$$
p(\theta|d) = \frac{\mathcal{L}(d|\theta)\pi(\theta)}{\color{red}{Z(d)}}= \frac{\mathcal{L}(d|\theta)\pi(\theta)}{\color{red}{\int_{\theta}\mathcal{L}(d|\theta)\pi(\theta) d\theta}}
$$</p><ul><li><em>Monte Carlo</em>: e.g. Rejection sampling</li><li><em>Markov-chain MC</em>: e.g. Metropolis-Hastings, NUTS</li><li><em>Variational Inference</em>: surrogate $p(\theta|d)$</li></ul><p><mark>What if we dont have $\mathcal{L}(d|\theta)$ ?</mark></p></section><section><h3 id=simulation-based-inference>Simulation based inference:</h3><p>New term for:</p><ul><li>Approximate Bayes Computation,</li><li>Likelihood free inference,</li><li>Indirect inference,</li><li>Synthetic likelihood</li></ul></section><section><h3 id=algorithm>Algorithm</h3><figure><div class="d-flex justify-content-center"><div class=w-100><img src=https://miro.medium.com/v2/1*oer83KfCCI1AnoqsRtYlRg.png alt loading=lazy data-zoomable width=400 height=400></div></div></figure><p>Compare the &lsquo;simulated&rsquo; data to the &rsquo;true&rsquo; data</p><aside class=notes><ul><li>Marginal inference &ndash; SBI its possible to directly target specific parameters for inference, ignore other parameters while still dealing correctly with the ones we dont care about</li><li>Amortized &ndash; SBI once trained &ndash; we can get answers of the posteriors very quickly</li></ul></aside></section><section><h3 id=different-sbi-methods>Different SBI methods:</h3><ul><li><strong>Classical</strong>: Rejection ABC (&lsquo;97), MCMC-ABC (&lsquo;03)</li><li><strong>Neural density</strong>:<ul><li>Neural posterior estimator</li><li>Neural likelihood estimator</li><li>Neural <em>ratio</em> estimator (Lnl/evid)</li></ul></li><li><strong>Types of NN:</strong><ul><li>Mixture density networks</li><li>Normalising flows</li></ul></li></ul></section><section><h3 id=goals-for-nn--sbi>Goals for NN + SBI:</h3><ul><li><em>Speed</em>: Training faster than MCMC</li><li><em>Scalability</em>: Doesn&rsquo;t fall apart with high D</li><li><em>Pre-existing research</em>: Leverage modern ML tools (flows, NNs &mldr;)</li></ul></section><section><h3 id=mcmc-vi-sbi>MCMC, VI, SBI</h3><table><thead><tr><th></th><th>MCMC</th><th>VI</th><th>SBI</th></tr></thead><tbody><tr><td>Explicit Likelihood</td><td>✅</td><td>✅</td><td>❌</td></tr><tr><td>Requires gradients</td><td>✅</td><td>(✅)</td><td>❌</td></tr><tr><td>Targeted inference</td><td>✅</td><td>✅</td><td>❌</td></tr><tr><td>Amortized</td><td>❌</td><td>(✅)</td><td>✅</td></tr><tr><td>Specialised architechture</td><td>❌</td><td>✅</td><td>✅</td></tr><tr><td>Requires data summaries</td><td>❌</td><td>❌</td><td>✅</td></tr><tr><td>Marginal inference</td><td>❌</td><td>❌</td><td>✅</td></tr></tbody></table><aside class=notes>Amortized posterior is one that is not focused on any particular observation</aside></section><section><h3 id=end-of-section>END OF SECTION</h3></section></section><section><section data-shortcode-section><h2 id=sbi-math>SBI Math</h2><p><strong>Skipping this, can come back if folks interested</strong></p><aside class=notes><p>Library: swyft
Simulation efficient marginal posterior estimation</p><p>Target: X</p><ul><li>say there are lots of parameters $\theta$</li><li>Only parameter values that plausiablly generate X will contribut to marginaliation</li><li>NESTED RATIO ESTIMATION finds this region by iteratively cnstraining the initial prior based on 1D marginal posteriors from previous iterations</li><li>this method approximates the likelihood-to-evidence ratio by zeroing in on the high-likelihood regions</li><li>method inspired by nested sampling</li><li>After a few iteraintins &ndash; some 1D marginals will be mre constrained than others</li></ul><p><a href="https://pbs.twimg.com/media/E65qN0dWEAAxXCW?format=png&amp;name=900x900" target=_blank rel=noopener>https://pbs.twimg.com/media/E65qN0dWEAAxXCW?format=png&name=900x900</a></p></aside></section><section><h3 id=d_kl-loss-function-for-training>$D_{KL}$ &ldquo;Loss&rdquo; function for training</h3><p>$$D_{\rm KL}(\tilde{p}, p) = \int \tilde{p}(x) \log \frac{\tilde{p}(x)}{p(x)}\ dx$$</p><p>$D_{KL}$ is <em>not</em> symmetric</p><ul><li>$D_{\rm KL}(\tilde{p}, p)$: Variational inference (LnL based)</li><li>$D_{\rm KL}(p, \tilde{p})$: NPE (Simulation based)</li></ul><p><strong>PROBLEM:</strong> how do we avoid evaluating the $p(\theta|d)$?</p></section><section><h3 id=kl-divergence-and-vi>KL-Divergence and VI</h3><p>$$D_{\rm KL} [\tilde{p}, p] (\theta) \sim \mathbb{E}_{\theta\sim\tilde{p}(\theta|d)} \log \left[ \frac{\tilde{p}(\theta|d)}{\mathcal{L}(d|\theta)\pi(\theta)} \right] + C$$</p><ul><li><strong>PROBLEM:</strong> $p(\theta|d)$ is $$$</li><li><strong>SOLUTION:</strong><ul><li>$p(\theta|d) \sim \mathcal{L}(d|\theta)\pi(\theta)$</li><li>$0\leq D_{\rm KL} [\tilde{p}, p]\leq Z(d)$</li><li>Train $\tilde{p}(\theta|d)$</li></ul></li></ul></section><section><h3 id=kl-divergence-and-sbi>KL-Divergence and SBI</h3><p>$$D_{\rm KL}[p, \tilde{p}] (\theta, d) \sim -\mathbb{E}_{(\theta,d)\sim p(\theta,d)} \log \tilde{p}(\theta| d) + C $$</p><ul><li><strong>PROBLEM:</strong> $p(\theta|d)$ is $$$</li><li><strong>SOLUTION:</strong><ul><li>sample from $p_{\rm joint}(\theta, d) = \mathcal{L}(d|\theta)\pi(\theta)$</li><li>Train $\tilde{p}(\theta|d)$</li></ul></li></ul></section><section><h3 id=marginal-sbi-vs-vi>Marginal SBI vs VI</h3><p><strong>Variatinal inference</strong></p><ul><li>variational posterior $\tilde{p}(\vec{\theta}|d)$ must conver <em>all</em> params likelihoodd model condditioned on</li></ul><p><strong>SBI Marginal inference</strong></p><ul><li>Can replace $\tilde{p}(\vec{\theta}|d)$ for $\tilde{p}(\theta_1|d)$ without need of doing integrals</li></ul></section><section><h3 id=end-of-section-1>END OF SECTION</h3></section></section><section data-noprocess data-shortcode-slide data-background-image=https://user-images.githubusercontent.com/15642823/277592172-be608f89-4e27-489f-b3ab-48011968790d.jpeg><h2 id=marginal-inference>&ldquo;Marginal&rdquo; inference</h2><p>$${\color{red}p(\theta_{\rm Waldo}| \rm{image})} =$$
$$\int {\color{blue}p(\theta_{A}, \theta_{B} &mldr; \theta_{\rm Waldo}| \rm{image})}\ d\theta_A\ d\theta_B\ d\theta_{\rm Waldo} $$</p><ul><li>VI: have to learn <em>whole</em> $\color{blue}p(\vec{\theta}|d)$</li><li>SBI: can focus on specific params $\color{red}p(\theta_{\rm Waldo}|d)$</li></ul></section><section><section data-shortcode-section><h2 id=truncated-marginal-neural-ratio-estimation-tmnre>Truncated Marginal Neural Ratio Estimation (TMNRE)</h2></section><section><h3 id=active-learning-loop>Active learning loop</h3><p><figure><div class="d-flex justify-content-center"><div class=w-100><img src=https://user-images.githubusercontent.com/15642823/277889707-8e9f5955-b8ac-44e0-8067-808a5ad189d2.png alt=loop loading=lazy data-zoomable></div></div></figure></p></section><section><h3 id=network-architecture>Network architecture</h3><p><figure><div class="d-flex justify-content-center"><div class=w-100><img src=https://user-images.githubusercontent.com/15642823/277868586-284becb9-8f47-4ed9-9a92-6a3e7683470d.png alt=network loading=lazy data-zoomable></div></div></figure></p></section><section><h3 id=truncation-example>Truncation example</h3><p><figure><div class="d-flex justify-content-center"><div class=w-100><img src=https://user-images.githubusercontent.com/15642823/277902380-7807ed9e-99ae-40c4-b242-b7e9328306ec.png alt=trunc loading=lazy data-zoomable></div></div></figure></p></section></section><section><section data-shortcode-section><h2 id=alvey-signal-and-noise-model>Alvey+ Signal and noise model</h2><ul><li><p>Noise model (only amplitudes parameterised &ndash; shape fixed):</p><ul><li>$\small S^{\rm N}(A, P, f) \sim A^2 s^{TM}(f) + P^2 s^{OMS}(f)$</li></ul></li><li><p>Two signal models (one chosen):</p><ul><li>$\tiny {\rm Power Law}: \Omega(\alpha, \gamma, f) \sim 10^\alpha\ f^\gamma$</li><li>$\tiny {\rm N-Power Laws}:\Omega(\vec{\alpha}, \vec{\gamma}, \vec{f}_{\rm range}, f) \sim \sum^N 10^\alpha_i\ f^\gamma_i\ \Theta[f_i^{\rm min}, f_i^{\rm max}]$</li></ul></li></ul></section><section><h3 id=base-model-consists-of>BASE Model consists of</h3><ul><li>data(t) = noise(t) + $\sum^{\rm signals}$ s_i(t)</li><li>Single TDI channel</li><li>12 days of data (split into 100 segments, 1 segment ~ 2.9 hours)</li><li>$\Delta f\sim0.1\ {\rm mHz}$</li></ul><aside class=notes>this is ~1% of the full LISA mission duration</aside></section><section><h3 id=model-with-transients>Model with transients:</h3><ul><li>Same as BASE mode</li><li>In each segement Inject 1 massive BH merger (priors below) if U[0,1] &lt; p</li></ul><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl>    <span class=n>Mc</span> <span class=o>=</span> <span class=n>U</span><span class=p>(</span><span class=mf>8e5</span><span class=p>,</span> <span class=mf>9e5</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>eta</span> <span class=o>=</span> <span class=n>U</span><span class=p>(</span><span class=mf>0.16</span><span class=p>,</span> <span class=mf>0.25</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>chi1</span> <span class=o>=</span> <span class=n>U</span><span class=p>(</span><span class=o>-</span><span class=mf>1.0</span><span class=p>,</span> <span class=mf>1.0</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>chi2</span> <span class=o>=</span> <span class=n>U</span><span class=p>(</span><span class=o>-</span><span class=mf>1.0</span><span class=p>,</span> <span class=mf>1.0</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>dist_mpc</span> <span class=o>=</span> <span class=n>U</span><span class=p>(</span><span class=mf>5e4</span><span class=p>,</span> <span class=mf>1e5</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>tc</span> <span class=o>=</span> <span class=mf>0.0</span>
</span></span><span class=line><span class=cl>    <span class=n>phic</span> <span class=o>=</span> <span class=mf>0.0</span>
</span></span></code></pre></div></section><section><h3 id=mla-training>MLA training:</h3><p>&ldquo;Several numerical settings should be chosen for the general structure of the algorithm as well as the network architechture&rdquo;</p><ul><li>500K simulations (9:1 train:val split)</li><li>50 epochs (512 batch size)</li><li>save model weights with the lowest validation loss</li></ul></section></section><section><section data-shortcode-section><h2 id=results--discussion>Results + Discussion</h2></section><section><h3 id=mcmc-vs-sbi-fit>MCMC vs SBI fit</h3><figure><div class="d-flex justify-content-center"><div class=w-100><img src=https://user-images.githubusercontent.com/15642823/277888874-1ab882f7-e3d1-47a9-a542-96101b8b92b5.png alt loading=lazy data-zoomable width=500px></div></div></figure></section><section><h3 id=some-thoughts>Some thoughts</h3><ul><li>The good:<ul><li>&lsquo;Implicit marginalisation&rsquo; may enable focused study (without global fit)!</li><li>Fewer evaluations of the model needed!</li></ul></li><li>The $\tiny{\rm bad}$ not so good:<ul><li>Doest use LnL even when known (no gradients)</li><li>Requires robust models for noise (<a href=https://avivajpeyi.github.io/lisa_notes/_images/runtime.png target=_blank rel=noopener>slow!</a>)</li><li>Need to model <em>all</em> signals in data generation?</li><li>MLA architecture&mldr;</li></ul></li><li>The ugly:<ul><li>unfair MCMC comparison for data with transients</li></ul></li></ul></section><section><h3 id=future-work>Future work</h3><ul><li>More complex noise model</li><li>Longer data duration</li><li>Additional data channels</li><li>other &ldquo;SBI&rdquo; blocks for the global fit</li></ul></section></section><section><h2 id=other-related-papers>Other related papers</h2><ul><li><a href=https://iopscience.iop.org/article/10.1088/1475-7516/2022/09/004/meta target=_blank rel=noopener>Fast and credible likelihood-free cosmology with TMNRE</a></li><li><a href=https://arxiv.org/abs/2009.11845 target=_blank rel=noopener>Improved reconstruction of a stochastic gravitational wave background with LISA</a></li><li><a href=https://arxiv.org/abs/2107.01214 target=_blank rel=noopener>Truncated Marginal Neural Ratio Estimation</a></li><li><a href=https://arxiv.org/abs/2308.08597 target=_blank rel=noopener>Scalable inference with Autoregressive Neural Ratio Estimation</a></li><li><a href=https://arxiv.org/pdf/2309.08430.pdf target=_blank rel=noopener>Fast Likelihood-free Reconstruction of Gravitational Wave Backgrounds</a></li><li><a href=https://github.com/avivajpeyi/dev_site/wiki/Materials-for-SBI-journal-club-presentation target=_blank rel=noopener>More plots</a></li></ul></section><section></section></div></div><script src=https://cdn.jsdelivr.net/npm/reveal.js@4.5.0/dist/reveal.min.js crossorigin=anonymous></script>
<script src=https://cdn.jsdelivr.net/npm/reveal.js@4.5.0/plugin/markdown/markdown.min.js crossorigin=anonymous></script>
<script src=https://cdn.jsdelivr.net/npm/reveal.js@4.5.0/plugin/notes/notes.min.js crossorigin=anonymous></script>
<script src=https://cdn.jsdelivr.net/npm/reveal.js@4.5.0/plugin/search/search.min.js crossorigin=anonymous></script>
<script src=https://cdn.jsdelivr.net/npm/reveal.js@4.5.0/plugin/math/math.min.js crossorigin=anonymous></script>
<script src=https://cdn.jsdelivr.net/npm/reveal.js@4.5.0/plugin/zoom/zoom.min.js crossorigin=anonymous></script>
<script src=https://cdn.jsdelivr.net/npm/reveal.js-menu@2.1.0/plugin.js integrity="sha256-M6JwAjnRAWmi+sbXURR/yAhWZKYhAw7YXnnLvIxrdGs=" crossorigin=anonymous></script>
<script src=https://cdn.jsdelivr.net/npm/reveal.js-menu@2.1.0/menu.js integrity="sha256-l14dklFcW5mWar6w/9KaW0fWVerf3mYr7Wt0+rXzFAA=" crossorigin=anonymous></script>
<link rel=stylesheet href=https://cdn.jsdelivr.net/npm/reveal.js-menu@2.1.0/menu.css integrity="sha256-0fU8HKLaTjgzfaV9CgSqbsN8ilA3zo6zK1M6rlgULd8=" crossorigin=anonymous><script src=/js/wowchemy-slides.js></script></body></html>