<!doctype html><html lang=en-us><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=generator content="Wowchemy 5.7.0 for Hugo"><link rel=manifest href=/manifest.webmanifest><link rel=icon type=image/png href=/media/icon_hu8bb3cd4a71e1097e37400a0f18d2a5e2_266862_32x32_fill_lanczos_center_3.png><link rel=apple-touch-icon type=image/png href=/media/icon_hu8bb3cd4a71e1097e37400a0f18d2a5e2_266862_192x192_fill_lanczos_center_3.png><link rel=canonical href=https://avivajpeyi.github.io/slides/journal_club/sbi_for_sgwb/><title>SBI for SGBW | Avi Vajpeyi</title><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/reveal.js@4.5.0/dist/reveal.min.css><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/reveal.js@4.5.0/dist/theme/white.min.css><link rel=stylesheet href=/css/libs/chroma/github-light.min.css></head><body><div class=reveal><div class=slides><section><h2 id=sbi-for-sgwb>SBI for SGWB</h2><p><em>Simulation based infernce for Stochastic GW background Analysis</em>
(Alvey et al, 2023)</p><p><a href=https://arxiv.org/pdf/2309.07954.pdf target=_blank rel=noopener>arxiv</a> | <a href=https://github.com/undark-lab/swyft target=_blank rel=noopener>swyft</a> | <a href=https://github.com/avivajpeyi/dev_site/edit/main/content/slides/journal_club/sbi_for_sgwb/index.md target=_blank rel=noopener>edit</a></p><p>NZ Gravity Journal Club</p><p>Oct 26th, 2023</p></section><section><h2 id=summary>Summary</h2><ol><li>Sim based inference</li><li>LISA &ldquo;Global fit&rdquo; + GW background</li><li>Alvey et al&rsquo;s LISA SGWB model</li><li>Results, Discussion + future work</li></ol></section><section><section data-shortcode-section><h2 id=sbi-intro>SBI Intro</h2></section><section><h3 id=traditional-problem>Traditional problem</h3><p>$$
p(\theta|d) = \frac{\mathcal{L}(d|\theta)\pi(\theta)}{\color{red}{Z(d)}}= \frac{\mathcal{L}(d|\theta)\pi(\theta)}{\color{red}{\int_{\theta}\mathcal{L}(d|\theta)\pi(\theta) d\theta}}
$$</p><ul><li><em>Monte Carlo</em>: e.g. Rejection sampling</li><li><em>Markov-chain MC</em>: e.g. Metropolis-Hastings, NUTS</li><li><em>Variational Inference</em>: surrogate $p(\theta|d)$</li></ul><p><mark>What if we dont have $\mathcal{L}(d|\theta)$ ?</mark></p></section><section><h3 id=simulation-based-inference>Simulation based inference:</h3><p>New term for:</p><ul><li>Approximate Bayes Computation,</li><li>Likelihood free inference,</li><li>Indirect inference,</li><li>Synthetic likelihood</li></ul></section><section><h3 id=algorithm>Algorithm</h3><figure><div class="d-flex justify-content-center"><div class=w-100><img src=https://miro.medium.com/v2/1*oer83KfCCI1AnoqsRtYlRg.png alt loading=lazy data-zoomable width=400 height=400></div></div></figure><p>Compare the &lsquo;simulated&rsquo; data to the &rsquo;true&rsquo; data</p><aside class=notes><ul><li>Marginal inference &ndash; SBI its possible to directly target specific parameters for inference, ignore other parameters while still dealing correctly with the ones we dont care about</li><li>Amortized &ndash; SBI once trained &ndash; we can get answers of the posteriors very quickly</li></ul></aside></section><section><h3 id=different-sbi-methods>Different SBI methods:</h3><ul><li><strong>Classical</strong>: Rejection ABC (&lsquo;97), MCMC-ABC (&lsquo;03)</li><li><strong>Neural density</strong>:<ul><li>Neural posterior estimator</li><li>Neural likelihood estimator</li><li>Neural <em>ratio</em> estimator (Lnl/evid)</li></ul></li><li><strong>Types of NN:</strong><ul><li>Mixture density networks</li><li>Normalising flows</li></ul></li></ul></section><section><h3 id=goals-for-nn--sbi>Goals for NN + SBI:</h3><ul><li><em>Speed</em>: Training faster than MCMC</li><li><em>Scalability</em>: Doesn&rsquo;t fall apart with high D</li><li><em>Pre-existing research</em>: Leverage modern ML tools (flows, NNs &mldr;)</li></ul></section><section><h3 id=mcmc-vi-sbi>MCMC, VI, SBI</h3><table><thead><tr><th></th><th>MCMC</th><th>VI</th><th>SBI</th></tr></thead><tbody><tr><td>Explicit Likelihood</td><td>✅</td><td>✅</td><td>❌</td></tr><tr><td>Requires gradients</td><td>✅</td><td>(✅)</td><td>❌</td></tr><tr><td>Targeted inference</td><td>✅</td><td>✅</td><td>❌</td></tr><tr><td>Amortized</td><td>❌</td><td>(✅)</td><td>✅</td></tr><tr><td>Specialised architechture</td><td>❌</td><td>✅</td><td>✅</td></tr><tr><td>Requires data summaries</td><td>❌</td><td>❌</td><td>✅</td></tr><tr><td>Marginal inference</td><td>❌</td><td>❌</td><td>✅</td></tr></tbody></table><aside class=notes>Amortized posterior is one that is not focused on any particular observation</aside></section><section><h3 id=end-of-section>END OF SECTION</h3></section></section><section><section data-shortcode-section><h2 id=sbi-math>SBI Math</h2><p><strong>Skipping this, can come back if folks interested</strong></p><aside class=notes><p>Library: swyft
Simulation efficient marginal posterior estimation</p><p>Target: X</p><ul><li>say there are lots of parameters $\theta$</li><li>Only parameter values that plausiablly generate X will contribut to marginaliation</li><li>NESTED RATIO ESTIMATION finds this region by iteratively cnstraining the initial prior based on 1D marginal posteriors from previous iterations</li><li>this method approximates the likelihood-to-evidence ratio by zeroing in on the high-likelihood regions</li><li>method inspired by nested sampling</li><li>After a few iteraintins &ndash; some 1D marginals will be mre constrained than others</li></ul><p><a href="https://pbs.twimg.com/media/E65qN0dWEAAxXCW?format=png&amp;name=900x900" target=_blank rel=noopener>https://pbs.twimg.com/media/E65qN0dWEAAxXCW?format=png&name=900x900</a></p></aside></section><section><h3 id=d_kl-loss-function-for-training>$D_{KL}$ &ldquo;Loss&rdquo; function for training</h3><p>$$D_{\rm KL}(\tilde{p}, p) = \int \tilde{p}(x) \log \frac{\tilde{p}(x)}{p(x)}\ dx$$</p><p>$D_{KL}$ is <em>not</em> symmetric</p><ul><li>$D_{\rm KL}(\tilde{p}, p)$: Variational inference (LnL based)</li><li>$D_{\rm KL}(p, \tilde{p})$: NPE (Simulation based)</li></ul><p><strong>PROBLEM:</strong> how do we avoid evaluating the $p(\theta|d)$?</p></section><section><h3 id=kl-divergence-and-vi>KL-Divergence and VI</h3><p>$$D_{\rm KL} [\tilde{p}, p] (\theta) \sim \mathbb{E}_{\theta\sim\tilde{p}(\theta|d)} \log \left[ \frac{\tilde{p}(\theta|d)}{\mathcal{L}(d|\theta)\pi(\theta)} \right] + C$$</p><ul><li><strong>PROBLEM:</strong> $p(\theta|d)$ is $$$</li><li><strong>SOLUTION:</strong><ul><li>$p(\theta|d) \sim \mathcal{L}(d|\theta)\pi(\theta)$</li><li>$0\leq D_{\rm KL} [\tilde{p}, p]\leq Z(d)$</li><li>Train $\tilde{p}(\theta|d)$</li></ul></li></ul></section><section><h3 id=kl-divergence-and-sbi>KL-Divergence and SBI</h3><p>$$D_{\rm KL}[p, \tilde{p}] (\theta, d) \sim -\mathbb{E}_{(\theta,d)\sim p(\theta,d)} \log \tilde{p}(\theta| d) + C $$</p><ul><li><strong>PROBLEM:</strong> $p(\theta|d)$ is $$$</li><li><strong>SOLUTION:</strong><ul><li>sample from $p_{\rm joint}(\theta, d) = \mathcal{L}(d|\theta)\pi(\theta)$</li><li>Train $\tilde{p}(\theta|d)$</li></ul></li></ul></section><section><h3 id=marginal-sbi-vs-vi>Marginal SBI vs VI</h3><p><strong>Variatinal inference</strong></p><ul><li>variational posterior $\tilde{p}(\vec{\theta}|d)$ must conver <em>all</em> params likelihoodd model condditioned on</li></ul><p><strong>SBI Marginal inference</strong></p><ul><li>Can replace $\tilde{p}(\vec{\theta}|d)$ for $\tilde{p}(\theta_1|d)$ without need of doing integrals</li></ul></section><section><h3 id=end-of-section-1>END OF SECTION</h3></section></section><section data-noprocess data-shortcode-slide data-background-image=https://user-images.githubusercontent.com/15642823/277592172-be608f89-4e27-489f-b3ab-48011968790d.jpeg><h2 id=marginal-inference>&ldquo;Marginal&rdquo; inference</h2><p>$${\color{red}p(\theta_{\rm Waldo}| \rm{image})} =$$
$$\int {\color{blue}p(\theta_{A}, \theta_{B} &mldr; \theta_{\rm Waldo}| \rm{image})}\ d\theta_A\ d\theta_B\ d\theta_{\rm Waldo} $$</p><ul><li>VI: have to learn <em>whole</em> $\color{blue}p(\vec{\theta}|d)$</li><li>SBI: can focus on specific params $\color{red}p(\theta_{\rm Waldo}|d)$</li></ul></section><section><section data-shortcode-section><h2 id=truncated-marginal-neural-ratio-estimation-tmnre>Truncated Marginal Neural Ratio Estimation (TMNRE)</h2></section><section><h3 id=goal>Goal</h3><ul><li>Leverage the marginal inference property of SBI</li><li>Compress the simulated data into a few summary statistics (similar to variational encoder)</li><li></li></ul></section><section><h3 id=active-learning-loop>Active learning loop</h3><p><figure><div class="d-flex justify-content-center"><div class=w-100><img src=https://user-images.githubusercontent.com/15642823/277889707-8e9f5955-b8ac-44e0-8067-808a5ad189d2.png alt=loop loading=lazy data-zoomable></div></div></figure></p></section><section><h3 id=network-architecture>Network architecture</h3><p><figure><div class="d-flex justify-content-center"><div class=w-100><img src=https://user-images.githubusercontent.com/15642823/277868586-284becb9-8f47-4ed9-9a92-6a3e7683470d.png alt=network loading=lazy data-zoomable></div></div></figure></p></section><section><h3 id=truncation-example>Truncation example</h3><p><figure><div class="d-flex justify-content-center"><div class=w-100><img src=https://user-images.githubusercontent.com/15642823/277902380-7807ed9e-99ae-40c4-b242-b7e9328306ec.png alt=trunc loading=lazy data-zoomable></div></div></figure></p></section><section><h3 id=end-of-section-2>END OF SECTION</h3></section></section><section><section data-shortcode-section><h2 id=lisa-data-analysis>LISA Data analysis</h2></section><section><h3 id=global-fit>Global Fit</h3></section><section><h3 id=sgwb-estimation-methods>SGWB estimation methods</h3></section><section><h3 id=end-of-section-3>END OF SECTION</h3></section></section><section><section data-shortcode-section><h2 id=alvey-et-als-sgwb-fit>Alvey et al&rsquo;s SGWB fit</h2></section><section><h3 id=signal-and-noise-model>Signal and noise model</h3><table><thead><tr><th style=text-align:center></th></tr></thead><tbody><tr><td style=text-align:center><figure><div class="d-flex justify-content-center"><div class=w-100><img src=https://user-images.githubusercontent.com/15642823/277888868-c7ac02f7-a2f1-4e49-ada3-f72c0e2eb71f.png alt=Signal loading=lazy data-zoomable></div></div></figure></td></tr><tr><td style=text-align:center><figure><div class="d-flex justify-content-center"><div class=w-100><img src=https://user-images.githubusercontent.com/15642823/277888887-50a16f75-854a-4098-b3d4-ca7a829a6324.png alt=Noise loading=lazy data-zoomable></div></div></figure></td></tr></tbody></table></section><section><h3 id=base-model-consists-of>BASE Model consists of</h3><ul><li>Data:<ul><li>2 paramter instrumental noise model (only amplitude &ndash; shape of noise curve is fixed)</li><li>SGWB model, either (a) or (b)</li><li>data(t) = noise(t) + Sum_signals s_i(t)</li></ul></li><li>Single TDI channel</li><li>12 days of data (split into 100 segments, 1 segment ~ 2.9 hours)</li><li>Freq resolution 1/2.9Hours ~ 0.1 mHz</li></ul><p>Note: this is ~1% of the full LISA mission duration</p></section><section><h3 id=model-with-transients>Model with transients:</h3><ul><li>Same as BASE mode</li><li>In each segement Inject 1 massive BH merger (priors below) if U[0,1] &lt; p</li></ul><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl>    <span class=n>Mc</span> <span class=o>=</span> <span class=n>U</span><span class=p>(</span><span class=mf>8e5</span><span class=p>,</span> <span class=mf>9e5</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>eta</span> <span class=o>=</span> <span class=n>U</span><span class=p>(</span><span class=mf>0.16</span><span class=p>,</span> <span class=mf>0.25</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>chi1</span> <span class=o>=</span> <span class=n>U</span><span class=p>(</span><span class=o>-</span><span class=mf>1.0</span><span class=p>,</span> <span class=mf>1.0</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>chi2</span> <span class=o>=</span> <span class=n>U</span><span class=p>(</span><span class=o>-</span><span class=mf>1.0</span><span class=p>,</span> <span class=mf>1.0</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>dist_mpc</span> <span class=o>=</span> <span class=n>U</span><span class=p>(</span><span class=mf>5e4</span><span class=p>,</span> <span class=mf>1e5</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>tc</span> <span class=o>=</span> <span class=mf>0.0</span>
</span></span><span class=line><span class=cl>    <span class=n>phic</span> <span class=o>=</span> <span class=mf>0.0</span>
</span></span></code></pre></div></section><section><h3 id=mla-training>MLA training:</h3><p>&ldquo;Several numerical settings should be chosen for the general structure of the algorithm as well as the network architechture&rdquo;</p><ul><li>500K simulations (9:1 train:val split)</li><li>50 epochs (512 batch size)</li><li>save model weights with the lowest validation loss</li></ul></section><section><h3 id=end-of-section-4>END OF SECTION</h3></section></section><section><section data-shortcode-section><h2 id=alvey-et-als-results--discussion>Alvey et al&rsquo;s Results + Discussion</h2></section><section><h3 id=mcmc-vs-sbi-fit>MCMC vs SBI fit</h3><p><figure><div class="d-flex justify-content-center"><div class=w-100><img src=https://user-images.githubusercontent.com/15642823/277888874-1ab882f7-e3d1-47a9-a542-96101b8b92b5.png alt=corner loading=lazy data-zoomable></div></div></figure></p></section><section><h3 id=future-work>Future work</h3><ul><li>comples noise model</li><li>longer data duration</li><li>other &ldquo;SBI&rdquo; blocks for the global fit</li></ul></section><section><h3 id=sbi-thoughts>SBI thoughts</h3><ul><li>Focused analysis and &lsquo;implicit marginalisation&rsquo; is neat!</li><li>Fewer evaluations of the model needed &ndash; good in sitations where the model is expensive to evaluate</li><li>doest use gradient information for lnL&mldr; sometimes we know the Lnl</li><li>How does SBI perform if all params are important?</li><li>What if the model for noise/SGWB is not the best?<ul><li>How would this work with a non-parametric model?</li></ul></li></ul></section><section><h3 id=end-of-section-5>END OF SECTION</h3></section></section><section><h2 id=other-related-papers>Other related papers</h2><ul><li><a href=https://iopscience.iop.org/article/10.1088/1475-7516/2022/09/004/meta target=_blank rel=noopener>Fast and credible likelihood-free cosmology with TMNRE</a></li><li><a href=https://arxiv.org/abs/2009.11845 target=_blank rel=noopener>Improved reconstruction of a stochastic gravitational wave background with LISA</a></li><li><a href=https://arxiv.org/abs/2107.01214 target=_blank rel=noopener>Truncated Marginal Neural Ratio Estimation</a></li><li><a href=https://arxiv.org/abs/2308.08597 target=_blank rel=noopener>Scalable inference with Autoregressive Neural Ratio Estimation</a></li><li><a href=https://arxiv.org/pdf/2309.08430.pdf target=_blank rel=noopener>Fast Likelihood-free Reconstruction of Gravitational Wave Backgrounds</a></li><li><a href=https://github.com/avivajpeyi/dev_site/wiki/Materials-for-SBI-journal-club-presentation target=_blank rel=noopener>More plots</a></li></ul></section><section></section></div></div><script src=https://cdn.jsdelivr.net/npm/reveal.js@4.5.0/dist/reveal.min.js crossorigin=anonymous></script>
<script src=https://cdn.jsdelivr.net/npm/reveal.js@4.5.0/plugin/markdown/markdown.min.js crossorigin=anonymous></script>
<script src=https://cdn.jsdelivr.net/npm/reveal.js@4.5.0/plugin/notes/notes.min.js crossorigin=anonymous></script>
<script src=https://cdn.jsdelivr.net/npm/reveal.js@4.5.0/plugin/search/search.min.js crossorigin=anonymous></script>
<script src=https://cdn.jsdelivr.net/npm/reveal.js@4.5.0/plugin/math/math.min.js crossorigin=anonymous></script>
<script src=https://cdn.jsdelivr.net/npm/reveal.js@4.5.0/plugin/zoom/zoom.min.js crossorigin=anonymous></script>
<script src=https://cdn.jsdelivr.net/npm/reveal.js-menu@2.1.0/plugin.js integrity="sha256-M6JwAjnRAWmi+sbXURR/yAhWZKYhAw7YXnnLvIxrdGs=" crossorigin=anonymous></script>
<script src=https://cdn.jsdelivr.net/npm/reveal.js-menu@2.1.0/menu.js integrity="sha256-l14dklFcW5mWar6w/9KaW0fWVerf3mYr7Wt0+rXzFAA=" crossorigin=anonymous></script>
<link rel=stylesheet href=https://cdn.jsdelivr.net/npm/reveal.js-menu@2.1.0/menu.css integrity="sha256-0fU8HKLaTjgzfaV9CgSqbsN8ilA3zo6zK1M6rlgULd8=" crossorigin=anonymous><script src=/js/wowchemy-slides.js></script></body></html>