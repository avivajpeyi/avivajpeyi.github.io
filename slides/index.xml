<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Slides | Avi Vajpeyi</title><link>https://avivajpeyi.github.io/slides/</link><atom:link href="https://avivajpeyi.github.io/slides/index.xml" rel="self" type="application/rss+xml"/><description>Slides</description><generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><item><title>SBI for SGBW</title><link>https://avivajpeyi.github.io/slides/journal_club/sbi_for_sgwb/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://avivajpeyi.github.io/slides/journal_club/sbi_for_sgwb/</guid><description>&lt;h2 id="sbi-for-sgwb">SBI for SGWB&lt;/h2>
&lt;p>&lt;em>Simulation based infernce for Stochastic GW background Analysis&lt;/em>
(Alvey et al, 2023)&lt;/p>
&lt;p>&lt;a href="https://arxiv.org/pdf/2309.07954.pdf" target="_blank" rel="noopener">arxiv&lt;/a> | &lt;a href="https://github.com/undark-lab/swyft" target="_blank" rel="noopener">swyft&lt;/a> | &lt;a href="https://github.com/avivajpeyi/dev_site/edit/main/content/slides/journal_club/sbi_for_sgwb/index.md" target="_blank" rel="noopener">edit&lt;/a>&lt;/p>
&lt;p>NZ Gravity Journal Club&lt;/p>
&lt;p>Oct 26th, 2023&lt;/p>
&lt;hr>
&lt;h2 id="summary">Summary&lt;/h2>
&lt;ol>
&lt;li>Sim based inference&lt;/li>
&lt;li>LISA &amp;ldquo;Global fit&amp;rdquo; + GW background&lt;/li>
&lt;li>Alvey et al&amp;rsquo;s LISA SGWB model&lt;/li>
&lt;li>Results, Discussion + future work&lt;/li>
&lt;/ol>
&lt;hr>
&lt;section data-shortcode-section>
&lt;h2 id="sbi-intro">SBI Intro&lt;/h2>
&lt;hr>
&lt;h3 id="traditional-problem">Traditional problem&lt;/h3>
&lt;p>$$
p(\theta|d) = \frac{\mathcal{L}(d|\theta)\pi(\theta)}{\color{red}{Z(d)}}= \frac{\mathcal{L}(d|\theta)\pi(\theta)}{\color{red}{\int_{\theta}\mathcal{L}(d|\theta)\pi(\theta) d\theta}}
$$&lt;/p>
&lt;ul>
&lt;li>&lt;em>Monte Carlo&lt;/em>: e.g. Rejection sampling&lt;/li>
&lt;li>&lt;em>Markov-chain MC&lt;/em>: e.g. Metropolis-Hastings, NUTS&lt;/li>
&lt;li>&lt;em>Variational Inference&lt;/em>: surrogate $p(\theta|d)$&lt;/li>
&lt;/ul>
&lt;p>&lt;mark>What if we dont have $\mathcal{L}(d|\theta)$ ?&lt;/mark>&lt;/p>
&lt;hr>
&lt;h3 id="simulation-based-inference">Simulation based inference:&lt;/h3>
&lt;p>New term for:&lt;/p>
&lt;ul>
&lt;li>Approximate Bayes Computation,&lt;/li>
&lt;li>Likelihood free inference,&lt;/li>
&lt;li>Indirect inference,&lt;/li>
&lt;li>Synthetic likelihood&lt;/li>
&lt;/ul>
&lt;hr>
&lt;h3 id="algorithm">Algorithm&lt;/h3>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="https://miro.medium.com/v2/1*oer83KfCCI1AnoqsRtYlRg.png" alt="" loading="lazy" data-zoomable width="400" height="400" />&lt;/div>
&lt;/div>&lt;/figure>
&lt;p>Compare the &amp;lsquo;simulated&amp;rsquo; data to the &amp;rsquo;true&amp;rsquo; data&lt;/p>
&lt;aside class="notes">
&lt;ul>
&lt;li>Marginal inference &amp;ndash; SBI its possible to directly target specific parameters for inference, ignore other parameters while still dealing correctly with the ones we dont care about&lt;/li>
&lt;li>Amortized &amp;ndash; SBI once trained &amp;ndash; we can get answers of the posteriors very quickly&lt;/li>
&lt;/ul>
&lt;/aside>
&lt;hr>
&lt;h3 id="different-sbi-methods">Different SBI methods:&lt;/h3>
&lt;ul>
&lt;li>&lt;strong>Classical&lt;/strong>: Rejection ABC (&amp;lsquo;97), MCMC-ABC (&amp;lsquo;03)&lt;/li>
&lt;li>&lt;strong>Neural density&lt;/strong>:
&lt;ul>
&lt;li>Neural posterior estimator&lt;/li>
&lt;li>Neural likelihood estimator&lt;/li>
&lt;li>Neural &lt;em>ratio&lt;/em> estimator (Lnl/evid)&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>Types of NN:&lt;/strong>
&lt;ul>
&lt;li>Mixture density networks&lt;/li>
&lt;li>Normalising flows&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;hr>
&lt;h3 id="goals-for-nn--sbi">Goals for NN + SBI:&lt;/h3>
&lt;ul>
&lt;li>&lt;em>Speed&lt;/em>: Training faster than MCMC&lt;/li>
&lt;li>&lt;em>Scalability&lt;/em>: Doesn&amp;rsquo;t fall apart with high D&lt;/li>
&lt;li>&lt;em>Pre-existing research&lt;/em>: Leverage modern ML tools (flows, NNs &amp;hellip;)&lt;/li>
&lt;/ul>
&lt;hr>
&lt;h3 id="mcmc-vi-sbi">MCMC, VI, SBI&lt;/h3>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>&lt;/th>
&lt;th>MCMC&lt;/th>
&lt;th>VI&lt;/th>
&lt;th>SBI&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>Explicit Likelihood&lt;/td>
&lt;td>✅&lt;/td>
&lt;td>✅&lt;/td>
&lt;td>❌&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Requires gradients&lt;/td>
&lt;td>✅&lt;/td>
&lt;td>(✅)&lt;/td>
&lt;td>❌&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Targeted inference&lt;/td>
&lt;td>✅&lt;/td>
&lt;td>✅&lt;/td>
&lt;td>❌&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Amortized&lt;/td>
&lt;td>❌&lt;/td>
&lt;td>(✅)&lt;/td>
&lt;td>✅&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Specialised architechture&lt;/td>
&lt;td>❌&lt;/td>
&lt;td>✅&lt;/td>
&lt;td>✅&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Requires data summaries&lt;/td>
&lt;td>❌&lt;/td>
&lt;td>❌&lt;/td>
&lt;td>✅&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Marginal inference&lt;/td>
&lt;td>❌&lt;/td>
&lt;td>❌&lt;/td>
&lt;td>✅&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;aside class="notes">
Amortized posterior is one that is not focused on any particular observation
&lt;/aside>
&lt;hr>
&lt;h3 id="end-of-section">END OF SECTION&lt;/h3>
&lt;/section>
&lt;hr>
&lt;section data-shortcode-section>
&lt;h2 id="sbi-math">SBI Math&lt;/h2>
&lt;p>&lt;strong>Skipping this, can come back if folks interested&lt;/strong>&lt;/p>
&lt;aside class="notes">
&lt;p>Library: swyft
Simulation efficient marginal posterior estimation&lt;/p>
&lt;p>Target: X&lt;/p>
&lt;ul>
&lt;li>say there are lots of parameters $\theta$&lt;/li>
&lt;li>Only parameter values that plausiablly generate X will contribut to marginaliation&lt;/li>
&lt;li>NESTED RATIO ESTIMATION finds this region by iteratively cnstraining the initial prior based on 1D marginal posteriors from previous iterations&lt;/li>
&lt;li>this method approximates the likelihood-to-evidence ratio by zeroing in on the high-likelihood regions&lt;/li>
&lt;li>method inspired by nested sampling&lt;/li>
&lt;li>After a few iteraintins &amp;ndash; some 1D marginals will be mre constrained than others&lt;/li>
&lt;/ul>
&lt;p>&lt;a href="https://pbs.twimg.com/media/E65qN0dWEAAxXCW?format=png&amp;amp;name=900x900" target="_blank" rel="noopener">https://pbs.twimg.com/media/E65qN0dWEAAxXCW?format=png&amp;name=900x900&lt;/a>&lt;/p>
&lt;/aside>
&lt;hr>
&lt;h3 id="d_kl-loss-function-for-training">$D_{KL}$ &amp;ldquo;Loss&amp;rdquo; function for training&lt;/h3>
&lt;p>$$D_{\rm KL}(\tilde{p}, p) = \int \tilde{p}(x) \log \frac{\tilde{p}(x)}{p(x)}\ dx$$&lt;/p>
&lt;p>$D_{KL}$ is &lt;em>not&lt;/em> symmetric&lt;/p>
&lt;ul>
&lt;li>$D_{\rm KL}(\tilde{p}, p)$: Variational inference (LnL based)&lt;/li>
&lt;li>$D_{\rm KL}(p, \tilde{p})$: NPE (Simulation based)&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>PROBLEM:&lt;/strong> how do we avoid evaluating the $p(\theta|d)$?&lt;/p>
&lt;hr>
&lt;h3 id="kl-divergence-and-vi">KL-Divergence and VI&lt;/h3>
&lt;p>$$D_{\rm KL} [\tilde{p}, p] (\theta) \sim \mathbb{E}_{\theta\sim\tilde{p}(\theta|d)} \log \left[ \frac{\tilde{p}(\theta|d)}{\mathcal{L}(d|\theta)\pi(\theta)} \right] + C$$&lt;/p>
&lt;ul>
&lt;li>&lt;strong>PROBLEM:&lt;/strong> $p(\theta|d)$ is $$$&lt;/li>
&lt;li>&lt;strong>SOLUTION:&lt;/strong>
&lt;ul>
&lt;li>$p(\theta|d) \sim \mathcal{L}(d|\theta)\pi(\theta)$&lt;/li>
&lt;li>$0\leq D_{\rm KL} [\tilde{p}, p]\leq Z(d)$&lt;/li>
&lt;li>Train $\tilde{p}(\theta|d)$&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;hr>
&lt;h3 id="kl-divergence-and-sbi">KL-Divergence and SBI&lt;/h3>
&lt;p>$$D_{\rm KL}[p, \tilde{p}] (\theta, d) \sim -\mathbb{E}_{(\theta,d)\sim p(\theta,d)} \log \tilde{p}(\theta| d) + C $$&lt;/p>
&lt;ul>
&lt;li>&lt;strong>PROBLEM:&lt;/strong> $p(\theta|d)$ is $$$&lt;/li>
&lt;li>&lt;strong>SOLUTION:&lt;/strong>
&lt;ul>
&lt;li>sample from $p_{\rm joint}(\theta, d) = \mathcal{L}(d|\theta)\pi(\theta)$&lt;/li>
&lt;li>Train $\tilde{p}(\theta|d)$&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;hr>
&lt;h3 id="marginal-sbi-vs-vi">Marginal SBI vs VI&lt;/h3>
&lt;p>&lt;strong>Variatinal inference&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>variational posterior $\tilde{p}(\vec{\theta}|d)$ must conver &lt;em>all&lt;/em> params likelihoodd model condditioned on&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>SBI Marginal inference&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>Can replace $\tilde{p}(\vec{\theta}|d)$ for $\tilde{p}(\theta_1|d)$ without need of doing integrals&lt;/li>
&lt;/ul>
&lt;hr>
&lt;h3 id="end-of-section-1">END OF SECTION&lt;/h3>
&lt;/section>
&lt;hr>
&lt;section data-noprocess data-shortcode-slide
data-background-image="https://user-images.githubusercontent.com/15642823/277592172-be608f89-4e27-489f-b3ab-48011968790d.jpeg"
>
&lt;h2 id="marginal-inference">&amp;ldquo;Marginal&amp;rdquo; inference&lt;/h2>
&lt;p>$${\color{red}p(\theta_{\rm Waldo}| \rm{image})} =$$
$$\int {\color{blue}p(\theta_{A}, \theta_{B} &amp;hellip; \theta_{\rm Waldo}| \rm{image})}\ d\theta_A\ d\theta_B\ d\theta_{\rm Waldo} $$&lt;/p>
&lt;ul>
&lt;li>VI: have to learn &lt;em>whole&lt;/em> $\color{blue}p(\vec{\theta}|d)$&lt;/li>
&lt;li>SBI: can focus on specific params $\color{red}p(\theta_{\rm Waldo}|d)$&lt;/li>
&lt;/ul>
&lt;hr>
&lt;section data-shortcode-section>
&lt;h2 id="truncated-marginal-neural-ratio-estimation-tmnre">Truncated Marginal Neural Ratio Estimation (TMNRE)&lt;/h2>
&lt;hr>
&lt;h3 id="goal">Goal&lt;/h3>
&lt;ul>
&lt;li>Leverage the marginal inference property of SBI&lt;/li>
&lt;li>Compress the simulated data into a few summary statistics (similar to variational encoder)&lt;/li>
&lt;li>&lt;/li>
&lt;/ul>
&lt;hr>
&lt;h3 id="active-learning-loop">Active learning loop&lt;/h3>
&lt;p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="https://user-images.githubusercontent.com/15642823/277889707-8e9f5955-b8ac-44e0-8067-808a5ad189d2.png" alt="loop" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/p>
&lt;hr>
&lt;h3 id="network-architecture">Network architecture&lt;/h3>
&lt;p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="https://user-images.githubusercontent.com/15642823/277868586-284becb9-8f47-4ed9-9a92-6a3e7683470d.png" alt="network" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/p>
&lt;hr>
&lt;h3 id="truncation-example">Truncation example&lt;/h3>
&lt;p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="https://user-images.githubusercontent.com/15642823/277902380-7807ed9e-99ae-40c4-b242-b7e9328306ec.png" alt="trunc" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/p>
&lt;hr>
&lt;h3 id="end-of-section-2">END OF SECTION&lt;/h3>
&lt;/section>
&lt;hr>
&lt;section data-shortcode-section>
&lt;h2 id="lisa-data-analysis">LISA Data analysis&lt;/h2>
&lt;hr>
&lt;h3 id="global-fit">Global Fit&lt;/h3>
&lt;hr>
&lt;h3 id="sgwb-estimation-methods">SGWB estimation methods&lt;/h3>
&lt;hr>
&lt;h3 id="end-of-section-3">END OF SECTION&lt;/h3>
&lt;/section>
&lt;hr>
&lt;section data-shortcode-section>
&lt;h2 id="alvey-et-als-sgwb-fit">Alvey et al&amp;rsquo;s SGWB fit&lt;/h2>
&lt;hr>
&lt;h3 id="signal-and-noise-model">Signal and noise model&lt;/h3>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th style="text-align:center">&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td style="text-align:center">
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="https://user-images.githubusercontent.com/15642823/277888868-c7ac02f7-a2f1-4e49-ada3-f72c0e2eb71f.png" alt="Signal" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:center">
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="https://user-images.githubusercontent.com/15642823/277888887-50a16f75-854a-4098-b3d4-ca7a829a6324.png" alt="Noise" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;hr>
&lt;h3 id="base-model-consists-of">BASE Model consists of&lt;/h3>
&lt;ul>
&lt;li>Data:
&lt;ul>
&lt;li>2 paramter instrumental noise model (only amplitude &amp;ndash; shape of noise curve is fixed)&lt;/li>
&lt;li>SGWB model, either (a) or (b)&lt;/li>
&lt;li>data(t) = noise(t) + Sum_signals s_i(t)&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Single TDI channel&lt;/li>
&lt;li>12 days of data (split into 100 segments, 1 segment ~ 2.9 hours)&lt;/li>
&lt;li>Freq resolution 1/2.9Hours ~ 0.1 mHz&lt;/li>
&lt;/ul>
&lt;p>Note: this is ~1% of the full LISA mission duration&lt;/p>
&lt;hr>
&lt;h3 id="model-with-transients">Model with transients:&lt;/h3>
&lt;ul>
&lt;li>Same as BASE mode&lt;/li>
&lt;li>In each segement Inject 1 massive BH merger (priors below) if U[0,1] &amp;lt; p&lt;/li>
&lt;/ul>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl"> &lt;span class="n">Mc&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">U&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mf">8e5&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mf">9e5&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">eta&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">U&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mf">0.16&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mf">0.25&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">chi1&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">U&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="mf">1.0&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mf">1.0&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">chi2&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">U&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="mf">1.0&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mf">1.0&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">dist_mpc&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">U&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mf">5e4&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mf">1e5&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">tc&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mf">0.0&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">phic&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mf">0.0&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;hr>
&lt;h3 id="mla-training">MLA training:&lt;/h3>
&lt;p>&amp;ldquo;Several numerical settings should be chosen for the general structure of the algorithm as well as the network architechture&amp;rdquo;&lt;/p>
&lt;ul>
&lt;li>500K simulations (9:1 train:val split)&lt;/li>
&lt;li>50 epochs (512 batch size)&lt;/li>
&lt;li>save model weights with the lowest validation loss&lt;/li>
&lt;/ul>
&lt;hr>
&lt;h3 id="end-of-section-4">END OF SECTION&lt;/h3>
&lt;/section>
&lt;hr>
&lt;section data-shortcode-section>
&lt;h2 id="alvey-et-als-results--discussion">Alvey et al&amp;rsquo;s Results + Discussion&lt;/h2>
&lt;hr>
&lt;h3 id="mcmc-vs-sbi-fit">MCMC vs SBI fit&lt;/h3>
&lt;p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="https://user-images.githubusercontent.com/15642823/277888874-1ab882f7-e3d1-47a9-a542-96101b8b92b5.png" alt="corner" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/p>
&lt;hr>
&lt;h3 id="future-work">Future work&lt;/h3>
&lt;ul>
&lt;li>comples noise model&lt;/li>
&lt;li>longer data duration&lt;/li>
&lt;li>other &amp;ldquo;SBI&amp;rdquo; blocks for the global fit&lt;/li>
&lt;/ul>
&lt;hr>
&lt;h3 id="sbi-thoughts">SBI thoughts&lt;/h3>
&lt;ul>
&lt;li>Focused analysis and &amp;lsquo;implicit marginalisation&amp;rsquo; is neat!&lt;/li>
&lt;li>Fewer evaluations of the model needed &amp;ndash; good in sitations where the model is expensive to evaluate&lt;/li>
&lt;li>doest use gradient information for lnL&amp;hellip; sometimes we know the Lnl&lt;/li>
&lt;li>How does SBI perform if all params are important?&lt;/li>
&lt;li>What if the model for noise/SGWB is not the best?
&lt;ul>
&lt;li>How would this work with a non-parametric model?&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;hr>
&lt;h3 id="end-of-section-5">END OF SECTION&lt;/h3>
&lt;/section>
&lt;hr>
&lt;h2 id="other-related-papers">Other related papers&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="https://iopscience.iop.org/article/10.1088/1475-7516/2022/09/004/meta" target="_blank" rel="noopener">Fast and credible likelihood-free cosmology with TMNRE&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://arxiv.org/abs/2009.11845" target="_blank" rel="noopener">Improved reconstruction of a stochastic gravitational wave background with LISA&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://arxiv.org/abs/2107.01214" target="_blank" rel="noopener">Truncated Marginal Neural Ratio Estimation&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://arxiv.org/abs/2308.08597" target="_blank" rel="noopener">Scalable inference with Autoregressive Neural Ratio Estimation&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://arxiv.org/pdf/2309.08430.pdf" target="_blank" rel="noopener">Fast Likelihood-free Reconstruction of Gravitational Wave Backgrounds&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/avivajpeyi/dev_site/wiki/Materials-for-SBI-journal-club-presentation" target="_blank" rel="noopener">More plots&lt;/a>&lt;/li>
&lt;/ul>
&lt;hr></description></item></channel></rss>