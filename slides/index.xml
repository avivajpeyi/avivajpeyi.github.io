<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Slides | Avi Vajpeyi</title><link>https://avivajpeyi.github.io/slides/</link><atom:link href="https://avivajpeyi.github.io/slides/index.xml" rel="self" type="application/rss+xml"/><description>Slides</description><generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><item><title>SBI for SGBW</title><link>https://avivajpeyi.github.io/slides/journal_club/sbi_for_sgwb/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://avivajpeyi.github.io/slides/journal_club/sbi_for_sgwb/</guid><description>&lt;h2 id="sbi-for-sgwb">SBI for SGWB&lt;/h2>
&lt;p>&lt;em>Simulation based infernce for Stochastic GW background Analysis&lt;/em>
(Alvey+, 2023)&lt;/p>
&lt;p>&lt;a href="https://arxiv.org/pdf/2309.07954.pdf" target="_blank" rel="noopener">arxiv&lt;/a> | &lt;a href="https://github.com/undark-lab/swyft" target="_blank" rel="noopener">swyft&lt;/a> | &lt;a href="https://github.com/avivajpeyi/dev_site/edit/main/content/slides/journal_club/sbi_for_sgwb/index.md" target="_blank" rel="noopener">edit&lt;/a>&lt;/p>
&lt;p>NZ Gravity Journal Club&lt;/p>
&lt;p>Oct 26th, 2023&lt;/p>
&lt;hr>
&lt;h2 id="summary">Summary&lt;/h2>
&lt;ol>
&lt;li>LISA &amp;ldquo;Global fit&amp;rdquo; + GW background&lt;/li>
&lt;li>Alvey+&amp;rsquo;s LISA SGWB model&lt;/li>
&lt;li>Sim based inference + TMNRE&lt;/li>
&lt;li>Results, Discussion + future work&lt;/li>
&lt;/ol>
&lt;hr>
&lt;section data-shortcode-section>
&lt;h2 id="lisa-data-analysis">LISA Data analysis&lt;/h2>
&lt;hr>
&lt;h3 id="the-data">The data&lt;/h3>
&lt;p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="https://github.com/avivajpeyi/dev_site/assets/15642823/af1e82e7-f1bc-4306-856e-b11e245cadf3" alt="lisa_data" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/p>
&lt;hr>
&lt;h3 id="the-global-fit">The &amp;ldquo;Global fit&amp;rdquo;&lt;/h3>
&lt;p>Analyze all the data, simultaneously, block-by-block&lt;/p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="https://github.com/avivajpeyi/dev_site/assets/15642823/1577656f-3c97-43e9-bc4d-7da09c6686ce" alt="" loading="lazy" data-zoomable width="1300" height="350" />&lt;/div>
&lt;/div>&lt;/figure>
&lt;p>$&amp;lt;10^5$ parameters in the full problem&lt;/p>
&lt;hr>
&lt;h3 id="sgwb-estimation-methods">SGWB estimation methods&lt;/h3>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Noise model&lt;/th>
&lt;th>Signal model&lt;/th>
&lt;th>Noise + Signal&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>&lt;a href="https://arxiv.org/abs/1906.09027" target="_blank" rel="noopener">Karnesis+ &amp;lsquo;19&lt;/a>&lt;/td>
&lt;td>&lt;a href="https://arxiv.org/abs/2302.12573" target="_blank" rel="noopener">Baghi+ &amp;lsquo;23&lt;/a>&lt;/td>
&lt;td>&lt;a href="https://arxiv.org/abs/2011.05055" target="_blank" rel="noopener">Boileau+ &amp;lsquo;20&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;a href="https://arxiv.org/abs/1906.09244" target="_blank" rel="noopener">Caprini+ &amp;lsquo;19&lt;/a>&lt;/td>
&lt;td>&lt;a href="https://arxiv.org/abs/2308.01056" target="_blank" rel="noopener">Muratore+ &amp;lsquo;23&lt;/a>&lt;/td>
&lt;td>&lt;a href="https://arxiv.org/abs/2303.15929" target="_blank" rel="noopener">Olaf+ &amp;lsquo;23&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;a href="https://arxiv.org/abs/2004.01135" target="_blank" rel="noopener">Pieroni+ &amp;lsquo;20&lt;/a>&lt;/td>
&lt;td>&lt;/td>
&lt;td>Aimen+ (WIP)&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>High precision reconstruction required to extract an SGWB signal&lt;/p>
&lt;/section>
&lt;hr>
&lt;h2 id="alveys-sbi-approach-motivations">Alvey+&amp;rsquo;s SBI approach motivations&lt;/h2>
&lt;aside class="notes">
&lt;ul>
&lt;li>Current SGWB approaches use stochastic sampling methods (MCMC, Nested sampling)&lt;/li>
&lt;li>These are not &lt;em>robust&lt;/em> to foreground transient signals (e.g. massive BH mergers)&lt;/li>
&lt;li>add more comlexities&lt;/li>
&lt;/ul>
&lt;/aside>
&lt;ol>
&lt;li>&amp;lsquo;Marginal inference&amp;rsquo; property&lt;/li>
&lt;li>Likelihood &amp;lsquo;free&amp;rsquo; inference&lt;/li>
&lt;li>More robust to foreground transient signals (e.g. massive BH mergers)&lt;/li>
&lt;/ol>
&lt;hr>
&lt;section data-shortcode-section>
&lt;h2 id="sbi">SBI&lt;/h2>
&lt;hr>
&lt;h3 id="traditional-problem">Traditional problem&lt;/h3>
&lt;p>$$
p(\theta|d) = \frac{\mathcal{L}(d|\theta)\pi(\theta)}{\color{red}{Z(d)}}= \frac{\mathcal{L}(d|\theta)\pi(\theta)}{\color{red}{\int_{\theta}\mathcal{L}(d|\theta)\pi(\theta) d\theta}}
$$&lt;/p>
&lt;ul>
&lt;li>&lt;em>Monte Carlo&lt;/em>: e.g. Rejection sampling&lt;/li>
&lt;li>&lt;em>Markov-chain MC&lt;/em>: e.g. Metropolis-Hastings, NUTS&lt;/li>
&lt;li>&lt;em>Variational Inference&lt;/em>: surrogate $p(\theta|d)$&lt;/li>
&lt;/ul>
&lt;p>&lt;mark>What if we dont have $\mathcal{L}(d|\theta)$ ?&lt;/mark>&lt;/p>
&lt;hr>
&lt;h3 id="simulation-based-inference">Simulation based inference:&lt;/h3>
&lt;p>New term for:&lt;/p>
&lt;ul>
&lt;li>Approximate Bayes Computation,&lt;/li>
&lt;li>Likelihood free inference,&lt;/li>
&lt;li>Indirect inference,&lt;/li>
&lt;li>Synthetic likelihood&lt;/li>
&lt;/ul>
&lt;hr>
&lt;h3 id="algorithm">Algorithm&lt;/h3>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="https://miro.medium.com/v2/1*oer83KfCCI1AnoqsRtYlRg.png" alt="" loading="lazy" data-zoomable width="400" height="400" />&lt;/div>
&lt;/div>&lt;/figure>
&lt;p>Compare the &amp;lsquo;simulated&amp;rsquo; data to the &amp;rsquo;true&amp;rsquo; data&lt;/p>
&lt;aside class="notes">
&lt;ul>
&lt;li>Marginal inference &amp;ndash; SBI its possible to directly target specific parameters for inference, ignore other parameters while still dealing correctly with the ones we dont care about&lt;/li>
&lt;li>Amortized &amp;ndash; SBI once trained &amp;ndash; we can get answers of the posteriors very quickly&lt;/li>
&lt;/ul>
&lt;/aside>
&lt;hr>
&lt;h3 id="different-sbi-methods">Different SBI methods:&lt;/h3>
&lt;ul>
&lt;li>&lt;strong>Classical&lt;/strong>: Rejection ABC (&amp;lsquo;97), MCMC-ABC (&amp;lsquo;03)&lt;/li>
&lt;li>&lt;strong>Neural density&lt;/strong>:
&lt;ul>
&lt;li>Neural posterior estimator&lt;/li>
&lt;li>Neural likelihood estimator&lt;/li>
&lt;li>Neural &lt;em>ratio&lt;/em> estimator (Lnl/evid)&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>Types of NN:&lt;/strong>
&lt;ul>
&lt;li>Mixture density networks&lt;/li>
&lt;li>Normalising flows&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;hr>
&lt;h3 id="goals-for-nn--sbi">Goals for NN + SBI:&lt;/h3>
&lt;ul>
&lt;li>&lt;em>Speed&lt;/em>: Training faster than MCMC&lt;/li>
&lt;li>&lt;em>Scalability&lt;/em>: Doesn&amp;rsquo;t fall apart with high D&lt;/li>
&lt;li>&lt;em>Pre-existing research&lt;/em>: Leverage modern ML tools (flows, NNs &amp;hellip;)&lt;/li>
&lt;/ul>
&lt;hr>
&lt;h3 id="mcmc-vi-sbi">MCMC, VI, SBI&lt;/h3>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>&lt;/th>
&lt;th>MCMC&lt;/th>
&lt;th>VI&lt;/th>
&lt;th>SBI&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>Explicit Likelihood&lt;/td>
&lt;td>✅&lt;/td>
&lt;td>✅&lt;/td>
&lt;td>❌&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Requires gradients&lt;/td>
&lt;td>✅&lt;/td>
&lt;td>(✅)&lt;/td>
&lt;td>❌&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Targeted inference&lt;/td>
&lt;td>✅&lt;/td>
&lt;td>✅&lt;/td>
&lt;td>❌&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Amortized&lt;/td>
&lt;td>❌&lt;/td>
&lt;td>(✅)&lt;/td>
&lt;td>✅&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Specialised architechture&lt;/td>
&lt;td>❌&lt;/td>
&lt;td>✅&lt;/td>
&lt;td>✅&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Requires data summaries&lt;/td>
&lt;td>❌&lt;/td>
&lt;td>❌&lt;/td>
&lt;td>✅&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Marginal inference&lt;/td>
&lt;td>❌&lt;/td>
&lt;td>❌&lt;/td>
&lt;td>✅&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;aside class="notes">
Amortized posterior is one that is not focused on any particular observation
&lt;/aside>
&lt;hr>
&lt;h3 id="end-of-section">END OF SECTION&lt;/h3>
&lt;/section>
&lt;hr>
&lt;section data-shortcode-section>
&lt;h2 id="sbi-math">SBI Math&lt;/h2>
&lt;p>&lt;strong>Skipping this, can come back if folks interested&lt;/strong>&lt;/p>
&lt;aside class="notes">
&lt;p>Library: swyft
Simulation efficient marginal posterior estimation&lt;/p>
&lt;p>Target: X&lt;/p>
&lt;ul>
&lt;li>say there are lots of parameters $\theta$&lt;/li>
&lt;li>Only parameter values that plausiablly generate X will contribut to marginaliation&lt;/li>
&lt;li>NESTED RATIO ESTIMATION finds this region by iteratively cnstraining the initial prior based on 1D marginal posteriors from previous iterations&lt;/li>
&lt;li>this method approximates the likelihood-to-evidence ratio by zeroing in on the high-likelihood regions&lt;/li>
&lt;li>method inspired by nested sampling&lt;/li>
&lt;li>After a few iteraintins &amp;ndash; some 1D marginals will be mre constrained than others&lt;/li>
&lt;/ul>
&lt;p>&lt;a href="https://pbs.twimg.com/media/E65qN0dWEAAxXCW?format=png&amp;amp;name=900x900" target="_blank" rel="noopener">https://pbs.twimg.com/media/E65qN0dWEAAxXCW?format=png&amp;name=900x900&lt;/a>&lt;/p>
&lt;/aside>
&lt;hr>
&lt;h3 id="d_kl-loss-function-for-training">$D_{KL}$ &amp;ldquo;Loss&amp;rdquo; function for training&lt;/h3>
&lt;p>$$D_{\rm KL}(\tilde{p}, p) = \int \tilde{p}(x) \log \frac{\tilde{p}(x)}{p(x)}\ dx$$&lt;/p>
&lt;p>$D_{KL}$ is &lt;em>not&lt;/em> symmetric&lt;/p>
&lt;ul>
&lt;li>$D_{\rm KL}(\tilde{p}, p)$: Variational inference (LnL based)&lt;/li>
&lt;li>$D_{\rm KL}(p, \tilde{p})$: NPE (Simulation based)&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>PROBLEM:&lt;/strong> how do we avoid evaluating the $p(\theta|d)$?&lt;/p>
&lt;hr>
&lt;h3 id="kl-divergence-and-vi">KL-Divergence and VI&lt;/h3>
&lt;p>$$D_{\rm KL} [\tilde{p}, p] (\theta) \sim \mathbb{E}_{\theta\sim\tilde{p}(\theta|d)} \log \left[ \frac{\tilde{p}(\theta|d)}{\mathcal{L}(d|\theta)\pi(\theta)} \right] + C$$&lt;/p>
&lt;ul>
&lt;li>&lt;strong>PROBLEM:&lt;/strong> $p(\theta|d)$ is $$$&lt;/li>
&lt;li>&lt;strong>SOLUTION:&lt;/strong>
&lt;ul>
&lt;li>$p(\theta|d) \sim \mathcal{L}(d|\theta)\pi(\theta)$&lt;/li>
&lt;li>$0\leq D_{\rm KL} [\tilde{p}, p]\leq Z(d)$&lt;/li>
&lt;li>Train $\tilde{p}(\theta|d)$&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;hr>
&lt;h3 id="kl-divergence-and-sbi">KL-Divergence and SBI&lt;/h3>
&lt;p>$$D_{\rm KL}[p, \tilde{p}] (\theta, d) \sim -\mathbb{E}_{(\theta,d)\sim p(\theta,d)} \log \tilde{p}(\theta| d) + C $$&lt;/p>
&lt;ul>
&lt;li>&lt;strong>PROBLEM:&lt;/strong> $p(\theta|d)$ is $$$&lt;/li>
&lt;li>&lt;strong>SOLUTION:&lt;/strong>
&lt;ul>
&lt;li>sample from $p_{\rm joint}(\theta, d) = \mathcal{L}(d|\theta)\pi(\theta)$&lt;/li>
&lt;li>Train $\tilde{p}(\theta|d)$&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;hr>
&lt;h3 id="marginal-sbi-vs-vi">Marginal SBI vs VI&lt;/h3>
&lt;p>&lt;strong>Variatinal inference&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>variational posterior $\tilde{p}(\vec{\theta}|d)$ must conver &lt;em>all&lt;/em> params likelihoodd model condditioned on&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>SBI Marginal inference&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>Can replace $\tilde{p}(\vec{\theta}|d)$ for $\tilde{p}(\theta_1|d)$ without need of doing integrals&lt;/li>
&lt;/ul>
&lt;hr>
&lt;h3 id="end-of-section-1">END OF SECTION&lt;/h3>
&lt;/section>
&lt;hr>
&lt;section data-noprocess data-shortcode-slide
data-background-image="https://user-images.githubusercontent.com/15642823/277592172-be608f89-4e27-489f-b3ab-48011968790d.jpeg"
>
&lt;h2 id="marginal-inference">&amp;ldquo;Marginal&amp;rdquo; inference&lt;/h2>
&lt;p>$${\color{red}p(\theta_{\rm Waldo}| \rm{image})} =$$
$$\int {\color{blue}p(\theta_{A}, \theta_{B} &amp;hellip; \theta_{\rm Waldo}| \rm{image})}\ d\theta_A\ d\theta_B\ d\theta_{\rm Waldo} $$&lt;/p>
&lt;ul>
&lt;li>VI: have to learn &lt;em>whole&lt;/em> $\color{blue}p(\vec{\theta}|d)$&lt;/li>
&lt;li>SBI: can focus on specific params $\color{red}p(\theta_{\rm Waldo}|d)$&lt;/li>
&lt;/ul>
&lt;hr>
&lt;section data-shortcode-section>
&lt;h2 id="truncated-marginal-neural-ratio-estimation-tmnre">Truncated Marginal Neural Ratio Estimation (TMNRE)&lt;/h2>
&lt;hr>
&lt;h3 id="active-learning-loop">Active learning loop&lt;/h3>
&lt;p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="https://user-images.githubusercontent.com/15642823/277889707-8e9f5955-b8ac-44e0-8067-808a5ad189d2.png" alt="loop" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/p>
&lt;hr>
&lt;h3 id="network-architecture">Network architecture&lt;/h3>
&lt;p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="https://user-images.githubusercontent.com/15642823/277868586-284becb9-8f47-4ed9-9a92-6a3e7683470d.png" alt="network" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/p>
&lt;hr>
&lt;h3 id="truncation-example">Truncation example&lt;/h3>
&lt;p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="https://user-images.githubusercontent.com/15642823/277902380-7807ed9e-99ae-40c4-b242-b7e9328306ec.png" alt="trunc" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;/p>
&lt;hr>
&lt;h3 id="end-of-section-2">END OF SECTION&lt;/h3>
&lt;/section>
&lt;hr>
&lt;section data-shortcode-section>
&lt;h2 id="alvey-signal-and-noise-model">Alvey+ Signal and noise model&lt;/h2>
&lt;ul>
&lt;li>
&lt;p>Noise model (only amplitudes parameterised &amp;ndash; shape fixed):&lt;/p>
&lt;ul>
&lt;li>$\small S^{\rm N}(A, P, f) \sim A^2 s^{TM}(f) + P^2 s^{OMS}(f)$&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>Two signal models (one chosen):&lt;/p>
&lt;ul>
&lt;li>$\tiny {\rm Power Law}: \Omega(\alpha, \gamma, f) \sim 10^\alpha\ f^\gamma$&lt;/li>
&lt;li>$\tiny {\rm N-Power Laws}:\Omega(\vec{\alpha}, \vec{\gamma}, \vec{f}_{\rm range}, f) \sim \sum^N 10^\alpha_i\ f^\gamma_i\ \Theta[f_i^{\rm min}, f_i^{\rm max}]$&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;hr>
&lt;h3 id="base-model-consists-of">BASE Model consists of&lt;/h3>
&lt;ul>
&lt;li>data(t) = noise(t) + $\sum^{\rm signals}$ s_i(t)&lt;/li>
&lt;li>Single TDI channel&lt;/li>
&lt;li>12 days of data (split into 100 segments, 1 segment ~ 2.9 hours)&lt;/li>
&lt;li>$\Delta f\sim0.1\ {\rm mHz}$&lt;/li>
&lt;/ul>
&lt;aside class="notes">
this is ~1% of the full LISA mission duration
&lt;/aside>
&lt;hr>
&lt;h3 id="model-with-transients">Model with transients:&lt;/h3>
&lt;ul>
&lt;li>Same as BASE mode&lt;/li>
&lt;li>In each segement Inject 1 massive BH merger (priors below) if U[0,1] &amp;lt; p&lt;/li>
&lt;/ul>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl"> &lt;span class="n">Mc&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">U&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mf">8e5&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mf">9e5&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">eta&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">U&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mf">0.16&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mf">0.25&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">chi1&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">U&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="mf">1.0&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mf">1.0&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">chi2&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">U&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="mf">1.0&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mf">1.0&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">dist_mpc&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">U&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mf">5e4&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mf">1e5&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">tc&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mf">0.0&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">phic&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mf">0.0&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;hr>
&lt;h3 id="mla-training">MLA training:&lt;/h3>
&lt;p>&amp;ldquo;Several numerical settings should be chosen for the general structure of the algorithm as well as the network architechture&amp;rdquo;&lt;/p>
&lt;ul>
&lt;li>500K simulations (9:1 train:val split)&lt;/li>
&lt;li>50 epochs (512 batch size)&lt;/li>
&lt;li>save model weights with the lowest validation loss&lt;/li>
&lt;/ul>
&lt;hr>
&lt;h3 id="end-of-section-3">END OF SECTION&lt;/h3>
&lt;/section>
&lt;hr>
&lt;section data-shortcode-section>
&lt;h2 id="results--discussion">Results + Discussion&lt;/h2>
&lt;hr>
&lt;h3 id="mcmc-vs-sbi-fit">MCMC vs SBI fit&lt;/h3>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="https://user-images.githubusercontent.com/15642823/277888874-1ab882f7-e3d1-47a9-a542-96101b8b92b5.png" alt="" loading="lazy" data-zoomable width="500px" />&lt;/div>
&lt;/div>&lt;/figure>
&lt;hr>
&lt;h3 id="some-thoughts">Some thoughts&lt;/h3>
&lt;ul>
&lt;li>The good:
&lt;ul>
&lt;li>&amp;lsquo;Implicit marginalisation&amp;rsquo; may enable focused study (without global fit)!&lt;/li>
&lt;li>Fewer evaluations of the model needed!&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>The ~bad~ not so good:
&lt;ul>
&lt;li>Doest use LnL even when known (no gradients)&lt;/li>
&lt;li>Requires robust models for noise (slow! &lt;a href="https://avivajpeyi.github.io/lisa_notes/_images/runtime.png" target="_blank" rel="noopener">look at this plot&lt;/a>)&lt;/li>
&lt;li>Need to model &lt;em>all&lt;/em> signals in data generation?&lt;/li>
&lt;li>MLA architecture&amp;hellip;&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>The ugly:
&lt;ul>
&lt;li>MCMC comparison for data with transients unfair?&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;hr>
&lt;h3 id="future-work">Future work&lt;/h3>
&lt;ul>
&lt;li>More complex noise model&lt;/li>
&lt;li>Longer data duration&lt;/li>
&lt;li>Additional data channels&lt;/li>
&lt;li>other &amp;ldquo;SBI&amp;rdquo; blocks for the global fit&lt;/li>
&lt;/ul>
&lt;hr>
&lt;h3 id="end-of-section-4">END OF SECTION&lt;/h3>
&lt;/section>
&lt;hr>
&lt;h2 id="other-related-papers">Other related papers&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="https://iopscience.iop.org/article/10.1088/1475-7516/2022/09/004/meta" target="_blank" rel="noopener">Fast and credible likelihood-free cosmology with TMNRE&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://arxiv.org/abs/2009.11845" target="_blank" rel="noopener">Improved reconstruction of a stochastic gravitational wave background with LISA&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://arxiv.org/abs/2107.01214" target="_blank" rel="noopener">Truncated Marginal Neural Ratio Estimation&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://arxiv.org/abs/2308.08597" target="_blank" rel="noopener">Scalable inference with Autoregressive Neural Ratio Estimation&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://arxiv.org/pdf/2309.08430.pdf" target="_blank" rel="noopener">Fast Likelihood-free Reconstruction of Gravitational Wave Backgrounds&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/avivajpeyi/dev_site/wiki/Materials-for-SBI-journal-club-presentation" target="_blank" rel="noopener">More plots&lt;/a>&lt;/li>
&lt;/ul>
&lt;hr></description></item></channel></rss>